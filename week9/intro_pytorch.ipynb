{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch and torchvision if you have not already done so\n",
    "# pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fac5d3bda50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic object used in PyTorch is the 'Tensor' which is equivalent to 'ndarray' in Numpy. Similar to Numpy, there are multiple types of Tensors, e.g. Float, Double, Int, Long, etc. Generally we will use FloatTensors, and it is the default type for most functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor manually\n",
    "x_manual = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "x_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "        [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "        [0.4556, 0.6323, 0.3489, 0.4017]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(3,4)\n",
    "print(x_ones)\n",
    "\n",
    "x_zeros = torch.zeros(3,4)\n",
    "print(x_zeros)\n",
    "\n",
    "x_uniform = torch.rand(3,4)\n",
    "print(x_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a NumPy array\n",
    "np_array = np.array([1., 2., 3.], dtype=np.float32)\n",
    "print(np_array)\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0223, 0.1689, 0.2939])\n",
      "[0.02232575 0.16885895 0.29388845]\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array from a tensor\n",
    "another_tensor = torch.rand(3)\n",
    "print(another_tensor)\n",
    "another_np_array = another_tensor.numpy()\n",
    "print(another_np_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0223, 0.1689, 0.2939],\n",
      "        [0.5185, 0.6977, 0.8000],\n",
      "        [0.1610, 0.2823, 0.6816]])\n",
      "tensor([0.1689, 0.6977, 0.2823])\n",
      "tensor([[0.0223, 0.1689, 0.2939],\n",
      "        [0.5185, 0.6977, 0.8000]])\n"
     ]
    }
   ],
   "source": [
    "# Use indexing to get slices from a tensor\n",
    "A = torch.rand(3,3)\n",
    "print(A)\n",
    "print(A[:, 1])\n",
    "print(A[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A+B\n",
      "tensor([[1.2203, 1.3291, 1.0501],\n",
      "        [0.6892, 0.7036, 0.9845],\n",
      "        [0.2443, 1.1150, 1.0965]])\n",
      "Elementwise multiplication\n",
      "tensor([[0.2792, 0.3701, 0.1538],\n",
      "        [0.1132, 0.0833, 0.0302],\n",
      "        [0.0075, 0.1722, 0.2700]])\n",
      "Dot product\n",
      "tensor([[0.5683, 1.7256, 0.8057],\n",
      "        [0.4754, 1.3601, 0.7802],\n",
      "        [0.1387, 0.4088, 0.2823]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(3,3)\n",
    "B = torch.rand(3,3)\n",
    "\n",
    "# Add tensors together\n",
    "print(\"A+B\")\n",
    "print(A+B)\n",
    "\n",
    "# Element-wise multiply tensors\n",
    "print(\"Elementwise multiplication\")\n",
    "print(A*B)\n",
    "\n",
    "# Matrix-Matrix multiplication of tensors\n",
    "print(\"Dot product\")\n",
    "print(torch.mm(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True\n",
    "else:\n",
    "    cuda = False\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3558, 0.1079, 0.0682,  ..., 0.5506, 0.5464, 0.1518],\n",
       "        [0.4867, 0.7275, 0.3683,  ..., 0.2273, 0.0821, 0.0421],\n",
       "        [0.7542, 0.9204, 0.3720,  ..., 0.9800, 0.4982, 0.0679],\n",
       "        ...,\n",
       "        [0.6559, 0.3565, 0.9990,  ..., 0.1482, 0.3409, 0.2187],\n",
       "        [0.8479, 0.3279, 0.9238,  ..., 0.7206, 0.4408, 0.1962],\n",
       "        [0.7649, 0.3776, 0.7856,  ..., 0.2745, 0.0313, 0.0507]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach a variable to the GPU\n",
    "mat_gpu = torch.rand(5000, 5000)\n",
    "if cuda:\n",
    "    mat_gpu = mat_gpu.cuda()\n",
    "mat_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "The key thing that PyTorch provides us is its Autograd capability which provides automatic differentiation. A Tensor keeps its value and the gradient with respect to this Tensor value. Almost all of built-in operations in PyTorch supports automatic differentiation. To use it we can call `.backward()` on a computation graph, e.g. neural network, after we finish our computation on the graph, and we can automatically get the accumulated gradient for each Tensor (which has specified `requires_grad=True`) in the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor(2.)\n",
      "w = tensor(0.5000, requires_grad=True)\n",
      "b = tensor(0.1000, requires_grad=True)\n",
      "y = tensor(1.1000, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=False)\n",
    "w = torch.tensor(0.5, requires_grad=True)\n",
    "b = torch.tensor(0.1, requires_grad=True)\n",
    "print('x =',x)\n",
    "print('w =',w)\n",
    "print('b =',b)\n",
    "\n",
    "# Define a computational graph\n",
    "y = w*x + b #y = 0.5x + 0.1 and y(2) = 1.1\n",
    "print('y =',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the gradient (derivative) of the above function y=wx+b with respect to our weight w and bias term b.  We can calculate them manually:\n",
    "\n",
    "For w:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial w} = \\frac{\\partial}{\\partial w}\\left(wx + b\\right) = x\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial w}\\Bigr|_{x=2} = 2 \n",
    "$$\n",
    "For b:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial b} = \\frac{\\partial}{\\partial b}\\left(wx + b\\right) = 1\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial b}\\Bigr|_{x=2} = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to w: tensor(2.)\n",
      "Gradient with respect to b: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients of y with respect to each variable x,w,b\n",
    "y.backward()\n",
    "\n",
    "print('Gradient with respect to w:',w.grad)\n",
    "print('Gradient with respect to b:',b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline_torch:\n",
    "    \n",
    "    def __init__(self,eta=0.01,n_iter=100,random_state=0):\n",
    "        self.eta=eta\n",
    "        self.n_iter=n_iter\n",
    "        self.random_state=random_state\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        P = X.shape[1] # Number of features\n",
    "        # Initialize the weights to small random numbers and be sure to set requires_grad=True\n",
    "        self.weights = torch.rand(P,requires_grad=True)\n",
    "        \n",
    "        # Train adaline using batch gradient descent\n",
    "        self.cost_path=[]\n",
    "        for i in range(self.n_iter):\n",
    "            # Zero out the weights gradient each iteration\n",
    "            if self.weights.grad is not None:\n",
    "                self.weights.grad.zero_()\n",
    "            # Calculate yhat\n",
    "            yhat = self.predict(X)\n",
    "            # Calculate the cost function - Sum of Squared Error\n",
    "            cost = torch.sum(0.5 * (y-yhat)**2)\n",
    "            # Gradient of cost function with respect to all weights\n",
    "            cost.backward()\n",
    "            # Update the weights. Gradient of weights is now stored in self.weights.grad\n",
    "            with torch.no_grad():\n",
    "                self.weights -= self.eta * self.weights.grad\n",
    "            # Add cost for the iteration to total cost counter\n",
    "            self.cost_path.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        yhat = torch.mv(X,self.weights)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data=load_breast_cancer(as_frame=True)\n",
    "X,y=data.data,data.target\n",
    "# Since the default in the file is 0=malignant 1=benign we want to reverse these\n",
    "y=(y==0).astype(int)\n",
    "X,y= np.array(X),np.array(y)\n",
    "\n",
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.2)\n",
    "\n",
    "# Let's scale our data to help the algorithm converge faster\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_scaled_tensor = torch.from_numpy(X_train_scaled).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "adaline_model = Adaline_torch(eta=0.0001,n_iter=100)\n",
    "adaline_model.fit(X_train_scaled_tensor,y_train_tensor)\n",
    "plt.plot(range(len(adaline_model.cost_path)),adaline_model.cost_path)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
