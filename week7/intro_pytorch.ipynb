{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch and torchvision if you have not already done so\n",
    "# pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fda33d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensors\n",
    "The basic object used in PyTorch is the 'Tensor' which is equivalent to 'ndarray' in Numpy. Similar to Numpy, there are multiple types of Tensors, e.g. Float, Double, Int, Long, etc. Generally we will use FloatTensors, and it is the default type for most functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor manually\n",
    "x_manual = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "x_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "        [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "        [0.4556, 0.6323, 0.3489, 0.4017]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(3,4)\n",
    "print(x_ones)\n",
    "\n",
    "x_zeros = torch.zeros(3,4)\n",
    "print(x_zeros)\n",
    "\n",
    "x_uniform = torch.rand(3,4)\n",
    "print(x_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a NumPy array\n",
    "np_array = np.array([1., 2., 3.], dtype=np.float32)\n",
    "print(np_array)\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0223, 0.1689, 0.2939])\n",
      "[0.02232575 0.16885895 0.29388845]\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array from a tensor\n",
    "another_tensor = torch.rand(3)\n",
    "print(another_tensor)\n",
    "another_np_array = another_tensor.numpy()\n",
    "print(another_np_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5185, 0.6977, 0.8000],\n",
      "        [0.1610, 0.2823, 0.6816],\n",
      "        [0.9152, 0.3971, 0.8742]])\n",
      "tensor([0.6977, 0.2823, 0.3971])\n",
      "tensor([[0.5185, 0.6977, 0.8000],\n",
      "        [0.1610, 0.2823, 0.6816]])\n"
     ]
    }
   ],
   "source": [
    "# Use indexing to get slices from a tensor\n",
    "A = torch.rand(3,3)\n",
    "print(A)\n",
    "print(A[:, 1])\n",
    "print(A[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A+B\n",
      "tensor([[0.6892, 0.7036, 0.9845],\n",
      "        [0.2443, 1.1150, 1.0965],\n",
      "        [1.0474, 1.4583, 0.4196]])\n",
      "\n",
      "Elementwise multiplication (Hadamard product)\n",
      "tensor([[0.1132, 0.0833, 0.0302],\n",
      "        [0.0075, 0.1722, 0.2700],\n",
      "        [0.2265, 0.4905, 0.0429]])\n",
      "\n",
      "Matrix multiplication (matrix product)\n",
      "tensor([[0.9355, 1.0787, 0.6453],\n",
      "        [0.3255, 0.3742, 0.2261],\n",
      "        [0.4069, 1.0051, 0.7265]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(3,3)\n",
    "B = torch.rand(3,3)\n",
    "\n",
    "# Add tensors together\n",
    "print(\"A+B\")\n",
    "print(A+B)\n",
    "\n",
    "# Element-wise multiply tensors\n",
    "print()\n",
    "print(\"Elementwise multiplication (Hadamard product)\")\n",
    "print(A*B)\n",
    "\n",
    "# Matrix-Matrix multiplication of tensors\n",
    "print()\n",
    "print(\"Matrix multiplication (matrix product)\")\n",
    "print(torch.mm(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True\n",
    "else:\n",
    "    cuda = False\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5846, 0.0332, 0.1387,  ..., 0.9534, 0.2357, 0.3334],\n",
       "        [0.8576, 0.6120, 0.8924,  ..., 0.3778, 0.3465, 0.4203],\n",
       "        [0.1008, 0.9075, 0.2329,  ..., 0.8757, 0.6707, 0.0709],\n",
       "        ...,\n",
       "        [0.9011, 0.0352, 0.5583,  ..., 0.3135, 0.2705, 0.3187],\n",
       "        [0.0967, 0.0548, 0.4999,  ..., 0.4541, 0.5116, 0.8959],\n",
       "        [0.6136, 0.4996, 0.0217,  ..., 0.3558, 0.1079, 0.0682]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach a variable to the GPU\n",
    "mat_gpu = torch.rand(5000, 5000)\n",
    "if cuda:\n",
    "    mat_gpu = mat_gpu.cuda()\n",
    "mat_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "The key thing that PyTorch provides us is its Autograd capability which provides automatic differentiation. A Tensor keeps its value and the gradient with respect to this Tensor value. Almost all of built-in operations in PyTorch supports automatic differentiation. To use it we can call `.backward()` on a computation graph, e.g. neural network, after we finish our computation on the graph, and we can automatically get the accumulated gradient for each Tensor (which has specified `requires_grad=True`) in the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor(2.)\n",
      "w = tensor(0.5000, requires_grad=True)\n",
      "b = tensor(0.1000, requires_grad=True)\n",
      "y = tensor(1.1000, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=False)\n",
    "w = torch.tensor(0.5, requires_grad=True)\n",
    "b = torch.tensor(0.1, requires_grad=True)\n",
    "print('x =',x)\n",
    "print('w =',w)\n",
    "print('b =',b)\n",
    "\n",
    "# Define a computational graph\n",
    "y = w*x + b #y = 0.5x + 0.1 and y(2) = 1.1\n",
    "print('y =',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the derivative of the above function y=wx+b with respect to our weight w and bias term b.  We can calculate them manually:\n",
    "\n",
    "For w:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial w} = \\frac{\\partial}{\\partial w}\\left(wx + b\\right) = x\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial w}\\Bigr|_{x=2} = 2 \n",
    "$$\n",
    "For b:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial b} = \\frac{\\partial}{\\partial b}\\left(wx + b\\right) = 1\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial b}\\Bigr|_{x=2} = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to w: tensor(2.)\n",
      "Gradient with respect to b: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Compute derivatives of y with respect to each variable x,w,b\n",
    "y.backward()\n",
    "\n",
    "print('Derivative with respect to w:',w.grad)\n",
    "print('Derivative with respect to b:',b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert y from tensor to a NumPy array\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# We get an error when we try this\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mtype\u001b[39m(y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# Convert y from tensor to a NumPy array\n",
    "# We get an error when we try this\n",
    "y = y.numpy()\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Must first detach y from the computational graph\n",
    "y = y.detach().numpy()\n",
    "type(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipi520_fall24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
